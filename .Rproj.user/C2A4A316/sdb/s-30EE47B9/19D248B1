{
    "collab_server" : "",
    "contents" : "#Subset land registry data for then using in a Processing sketch\n#Subset by TTWA to get cities and city regions\nlibrary(plyr)\nlibrary(readr)\n#Memory checking\nlibrary(pryr)\nlibrary(zoo)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\ngeolibs <- c(\"ggmap\",\"rgdal\",\"rgeos\",\"maptools\",\"dplyr\",\"tidyr\",\"tmap\",\"raster\")\nlapply(geolibs, library, character.only = TRUE)\n\n#land registry data - there's a lot of it! This may not open...\n# lrd <- read.csv(\"C:/Data/LandRegistry_UKpricePaids1995toNow/pp-complete.csv\")\n\n#That doesn't work! Let's try a yearly amount instead, just to look at the data\n#No header\n# lrd <- read.csv(\"C:/Data/LandRegistry_pricePaidsYearly/pp-2015.csv\")\n\n#OK, just got to three columns and much smaller file. Can we open it?\n#Pretty sure I've opened bigger files than this!\n#lrd <- read.csv(\"pp-reducedColumns.csv\",header=F)\n\n#Subset just to Sheffield now before geocoding by postcode\n#lrd_shf <- lrd[lrd$V4==\"SHEFFIELD\",]\n\n#Or just do that in delimit, now I've noticed it's all caps...\nshef <- read.csv(\"pp-sheffield.csv\", header=F)\n\n#Get open codepoint postcode directory, just for Sheffield for now\npcs <- read.csv(\"C:/Data/Postcodes/Data/CSV/s.csv\", header=F)\n\n#NUmbers of spaces differ in both files' pcodes I think.\npcs$pcode_singleSpace <- gsub(\"  \",\" \",pcs$V1)\n\n#check match number\n#i suspect earlier postcodes matching less\n#As usual, enough to look at price trends I woulda thunk\n#FALSE  TRUE \n#137 10681 \ntable(unique(shef$V4) %in% unique(pcs$pcode_singleSpace))\n\ngeocoded <- merge(shef, pcs[,c(\"pcode_singleSpace\",\"V3\",\"V4\")], by.x=\"V4\", by.y=\"pcode_singleSpace\")\n\n#Let's get the columns we need for now and some sensible names\nnames(geocoded)[1] <- \"pcode\"\ngeocoded <- geocoded %>% dplyr::select(1,3,4,5,17,18)\n\nnames(geocoded) <- c('pcode','price','date','type','eastings','northings')\n\nwrite.csv(geocoded,\"sheffield-geocodedPostcodes-landRegistry.csv\", row.names = F)\n\ngeocoded <- read_csv(\"sheffield-geocodedPostcodes-landRegistry.csv\")\n\n#Process dates and set in date order ready for use\n#1. Get rid of time field. All midnight. What were they thinking??\n# testdate <- as.data.frame(geocoded$date[grep(\" 00:00\",geocoded$date)])#same obs as geocoded\n# testdate <- as.data.frame(geocoded$date[grep(\" 00:00\",geocoded$date, invert=T)])#zero\ngeocoded$newdate <- gsub(\" 00:00\",\"\",geocoded$date)\n\ngeocoded$newdate <- as.Date(geocoded$newdate, format=\"%Y-%m-%d\")\n\n#order earliest date first\ngeocoded <- geocoded[order(geocoded$newdate),]\n\n#Add in screen-friendly date (yearmon from zoo package)\ngeocoded$yearmon <- as.yearmon(geocoded$newdate)\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#Look at sheffield prices\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n#Just check on general sheffield price change, average per year\ngeocoded$year <- geocoded$newdate\ngeocoded$year <- format(geocoded$year,\"%Y\")\n\nyearAvs <- geocoded %>% group_by(year) %>% \n  summarise(mean = mean(price))\n\nyearAvs$percentChange <- 0\nyearAvs$percentChange <- (yearAvs$mean/lag(yearAvs$mean,1)-1)*100\n\n#Mark some Sheffield sub-geographies\nshefwards <- readOGR(dsn=\"QGIS\",layer=\"sheffield_wards\")\n\ngeocoded_geo <- geocoded\ncoordinates(geocoded_geo) <- ~eastings+northings\n\nproj4string(geocoded_geo) <- proj4string(shefwards)\n\nsalesinward <- geocoded_geo %over% shefwards\n\n#Will be (should be?) in same order\ngeocoded$ward <- salesinward$NAME\n\n#Filter out the Os, which I think is \"other\"\ngeocoded <- geocoded[geocoded$type!=\"O\",]\n\n\n#Some (probably in richer rural areas) clearly not enough sales to show anything useful\n#Filter.\ncounts <- geocoded %>% group_by(ward) %>% \n  summarise(countperward = n()) %>% \n  arrange(countperward)\n\n#Drop those silly small number of sales\ngeocoded2 <- geocoded %>% group_by(ward) %>% \n  mutate(countperward = n()) %>% \n  filter(countperward >4000)\n\n\n\nyearwardavs <- geocoded2 %>% group_by(year, ward) %>% \n  summarise(mean = median(price))\n  # summarise(mean = mean(price))\n\n#add in sheffield-wide average\nshefwide <- geocoded2 %>% group_by(year) %>% \n  summarise(mean = median(price))\n  # summarise(mean = mean(price))\n\nshefwide$ward <- \"all_Sheffield\"\n\nyearwardavs <- rbind(yearwardavs,shefwide)\n\n#Let's just remove 2016 - not enough data for accurate yearly average given seasonal wotsits\nyearwardavs <- yearwardavs[yearwardavs$year!=2016,]\n\n#Plot em all just to see spread\noutput <- ggplot(yearwardavs) +\n  geom_line(data =yearwardavs, \n            aes(x = as.Date(year, format = \"%Y\"), y = mean, colour = ward)) +\n  # scale_y_log10() +\n  guides(colour = F)\n\noutput\n\n#top and bottom based on final price\n#yearwardavs <- yearwardavs[order(yearwardavs$mean),]\n\nyearwardavs <- geocoded2 %>% group_by(year, ward) %>% \n  summarise(mean = mean(price)) %>% \n  #summarise(mean = median(price)) %>% \n  filter(year!=2016)\n\n\n#Won't necessarily be the final year but will do the job\nyearwardavs <- yearwardavs %>% group_by(ward) %>% \n  mutate(maxval = max(mean))\n\n#Get unique list of all those final mean prices\nmeanz <- unique(yearwardavs$maxval) %>% sort\n\n#cbPalette <- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nyearwardavs$ward <- reorder(yearwardavs$ward, -yearwardavs$maxval)\n\noutput <- ggplot() +\n  geom_line(data =yearwardavs[yearwardavs$maxval %in% meanz[c(1:4,(length(meanz)-3):length(meanz))],], \n            aes(x = as.Date(year, format = \"%Y\"), y = mean, colour = ward)) +\n  # scale_colour_manual(values=cbPalette)\n  scale_y_log10()\n#  guides(colour = F)\n\noutput\n\nmeanz[c(1:5,6)]\n\n\noutput <- ggplot() +\n  geom_line(data =yearwardavs[yearwardavs$ward %in% c('Burngreave','Nether Edge'),], \n            aes(x = as.Date(year, format = \"%Y\"), y = mean, colour = ward)) +\n scale_y_log10()\n\noutput\n\n\noutput <- ggplot() +\n  geom_line(data =yearwardavs[yearwardavs$ward %in% c('Burngreave','Nether Edge','Manor Castle','all_Sheffield'),], \n            aes(x = as.Date(year, format = \"%Y\"), y = mean, colour = ward)) +\n  xlab('year') +\n  # annotation_logticks(sides = 'lr') +\n  scale_y_log10() \n  \noutput\n\n\n#Look at price variation in burngreave for type of property\nburng <- geocoded %>% filter(ward == 'Burngreave') %>% \n  group_by(year, type) %>% \n  summarise(mean = mean(price))\n\n\noutput <- ggplot(burng) +\n  geom_line(data =burng, \n            aes(x = as.Date(year, format = \"%Y\"), y = mean, colour = type)) +\n  xlab('year') +\n  # annotation_logticks(sides = 'lr') +\n  scale_y_log10() \n\noutput\n\n\nburng <- geocoded %>% filter(ward == 'Dore and Totley') %>% \n  group_by(year, type) %>% \n  summarise(mean = mean(price))\n\n\noutput <- ggplot(burng) +\n  geom_line(data =burng, \n            aes(x = as.Date(year, format = \"%Y\"), y = mean, colour = type)) +\n  xlab('year') +\n  # annotation_logticks(sides = 'lr') +\n  scale_y_log10() \n\noutput\n\nburng <- geocoded %>% filter(ward == 'Manor Castle') %>% \n  group_by(year, type) %>% \n  summarise(mean = mean(price))\n\n\noutput <- ggplot(burng) +\n  geom_line(data =burng, \n            aes(x = as.Date(year, format = \"%Y\"), y = mean, colour = type)) +\n  xlab('year') +\n  # annotation_logticks(sides = 'lr') +\n  scale_y_log10() \n\noutput\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#Sheffield prices decomposition----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nshefts <- shef[shef$V5!='O',c('V2','V3')]\nnames(shefts) <- c('price','date')\n\nshefts$newdate <- gsub(\" 00:00\",\"\",shefts$date)\n\nshefts$newdate <- as.Date(shefts$newdate, format=\"%Y-%m-%d\")\nshefts$yearmon <- as.yearmon(shefts$newdate)\n\n#stick into month bins for time-series-ing\nsheftsmon <- shefts %>% group_by(yearmon) %>% \n  summarise(mean = mean(price))\n\n\n#Just plot to look for seasonal trend directly\noutput <- ggplot() +\n  geom_line(data = sheftsmon %>% filter(yearmon>1994&yearmon<1999), aes(x = as.Date(yearmon), y = mean))\n  # geom_line(data = sheftsmon %>% filter(yearmon>2006&yearmon<2016), aes(x = as.Date(yearmon), y = mean))\n\noutput\n\nshefts.ts <- ts(sheftsmon, frequency = 12, start = c(1995,1), end = c(2016,7))\n\nplot(shefts.ts)\n\nfitthing <- window(shefts.ts, start = c(1995,1), end = c(2016,7))\nfit <- stl(fitthing[,2],s.window = 'periodic')\nplot(fit)\n\nfitthing <- window(shefts.ts, start = c(2012,1), end = c(2014,12))\nfit <- stl(fitthing[,2],s.window = 'periodic')\nplot(fit)\n\nfit$time.series\n\n\n####################\n#Old subsetting----\n#OK. Now I can do some subsetting. I think the postcode are national grid refs so we should already have a match\n#But we'll need to spatial-points it up.\n\n#load geocoded land registry exchanges\ngeocoded <- readRDS(\"C:/Data/LandRegistry_UKpricePaids1995toNow/geocoded_landRegistrypricePaids_missingfiftythousand.rds\")\n\n#Get ttwas\nttwas <- readOGR(dsn=\"C:/Data/MapPolygons/England/2001/England_ttwa_2001\", \n                 layer=\"england_ttwa_2001\")\n\n#spatial-y-ize\ncoordinates(geocoded) <- as.matrix(geocoded[,c(3,4)])\n\n#might save that, it took a while...\nsaveRDS(geocoded,\"C:/Data/LandRegistry_UKpricePaids1995toNow/coords_added_geocoded_landRegistrypricePaids_missingfiftythousand.rds\")\n\n##########################\n#load spatialyized version\ngeocoded <- readRDS(\"C:/Data/LandRegistry_UKpricePaids1995toNow/coords_added_geocoded_landRegistrypricePaids_missingfiftythousand.rds\")\n\n#Tell R they're currently British National Grid\nproj4string(geocoded) <- CRS(\"+init=epsg:27700\")\n\n#Need to match CRS for pip test later...\nproj4string(ttwas) <- CRS(\"+init=epsg:27700\")\n\n#Get sample of those points for checking via plot\ngeocsample <- geocoded[sample(1:nrow(geocoded),size=1000),]\n\nplot(ttwas)\npoints(geocsample, col=\"green\")\n#Yup, we have same geography - plenty in Wales too but not gonna get those now.\n\n#Now to cycle through all the TTWAs\n#do point in polygon test\n#Save result as separate data\n#London (index 102 in 'ttwas') takes a VERY long time - let's not run it again!\n# for(i in 1:nrow(ttwas)){\n\n#Let's pick out a list of cities.\n#Bristol has two TTWAs, will need to deal with separately\n#As is Oxford\ncities <- c(20,26,53,59,83,98,99,109,125,158,162,203)\n\n# for(i in c(1:101,103:nrow(ttwas))){\nfor(i in cities){\n# for(i in 102){#london\n# for(i in 203){#york\n  \n  points <- geocoded[ttwas[i,],]\n  \n  #Order first by price, to add a price order index in\n  # points <- points[order(points@data$price),]\n  # points$priceindex <- seq(1:nrow(points))\n  \n  #Then order back to date for Processing\n  # points <- points[order(points@data$date),]\n  \n  #Or just add a rank column so same prices are same\n  points$priceindex <- rank(points@data$price)\n  \n  #write as CSV\n  write.csv(points@data[,c(\"postcode\",\"price\",\"oseast1m\",\"osnrth1m\",\"date\",\"yearmon\", \"priceindex\")],\n            paste(\"C:/Data/LandRegistry_UKpricePaids1995toNow/GeocodedTTWAs/TTWA_\",\n                  ttwas@data[i,c(1)],\"_\",\n                  ttwas@data[i,c(2)],\".csv\",sep=\"\"), row.names=F)\n  \n  \n}\n\n#Add TTWA details to geocoded for other stats. Can subset/save from that\n#I'll come back to that!\n# geocoded$ttwa <- 0\n# geocoded$ttwa_name <- \"\"\n# \n# for(i in c(1:101,103:nrow(ttwas))){\n#   \n#   geocoded[ttwas[i,],]$ttwa <- ttwas@data[i,c(1)]\n#   \n#   #write as CSV\n#   write.csv(points@data[,c(\"postcode\",\"price\",\"oseast1m\",\"osnrth1m\",\"date\",\"yearmon\")],\n#             paste(\"C:/Data/LandRegistry_UKpricePaids1995toNow/GeocodedTTWAs/TTWA_\",\n#                   ttwas@data[i,c(1)],\"_\",\n#                   ttwas@data[i,c(2)],\".csv\",sep=\"\"), row.names=F)\n#   \n#   \n# }\n\n#Test London points\nlon <- read.csv(\"C:/Data/LandRegistry_UKpricePaids1995toNow/GeocodedTTWAs/KEEP_TTWA_135_London.csv\")\ncoordinates(lon) <- as.matrix(lon[,c(3,4)])\nproj4string(lon) <- CRS(\"+init=epsg:27700\")\nplot(ttwas[102,])\nlonsamp <- lon[sample(1:nrow(lon),size=10000),]\npoints(lonsamp,col=\"green\")\n#Tick!\n\n#have a look at some summary stats for thinking about how we map prices\nlonplot <- as.data.frame(lon@data$price)\ncolnames(lonplot)[1] <- \"price\"\n\noutput <- ggplot(lonplot, aes(x = price)) +\n  geom_histogram() +\n  scale_x_log10(limits = c(8000, 10000000))\n  \n  # coord_cartesian(xlim = c(0, 10000000)) \n\noutput\n\n#Test some others\nandover <- read.csv(\"C:/Data/LandRegistry_UKpricePaids1995toNow/GeocodedTTWAs/TTWA_3_Andover.csv\")\n\n# coordinates(lon) <- as.matrix(lon[,c(3,4)])\n# proj4string(lon) <- CRS(\"+init=epsg:27700\")\n# plot(ttwas[102,])\n# lonsamp <- lon[sample(1:nrow(lon),size=10000),]\n# points(lonsamp,col=\"green\")\n#Tick!\n\nmean(andover$price)\nsd(andover$price)\nquantile(andover$price)\nquantile(lon$price)\n\n#have a look at some summary stats for thinking about how we map prices\nlonplot <- as.data.frame(lon@data$price)\ncolnames(lonplot)[1] <- \"price\"\n\noutput <- ggplot(lonplot, aes(x = price)) +\n  geom_histogram() +\n  scale_x_log10()\n\noutput\n\n\n#clear memory\nrm(list = ls(all = TRUE))\n\n#test ranking\nt <- c(7,5,3,6,5,5,4,4,4,1,6,7,7,8,9)\nr <- rank(t, ties.method = \"average\")#which is the default method I think\n\n\n\n\n\n\n",
    "created" : 1479574325810.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3022968421",
    "id" : "19D248B1",
    "lastKnownWriteTime" : 1479121502,
    "last_content_update" : 1479121502,
    "path" : "C:/Data/Housing/LandRegistry/subsetLandRegistryData.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}